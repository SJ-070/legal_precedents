"""
ì„±ëŠ¥ í…ŒìŠ¤íŠ¸: TF-IDF ë²¡í„°í™” ë°©ì‹ ë¹„êµ
- Word-based TF-IDF
- Character n-gram TF-IDF
- Hybrid (Word + Char)

ì‹¤í–‰: python test_vectorization_performance.py
"""

import json
import time
import tracemalloc
from typing import List, Dict, Tuple
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re

# ë²•ë¥  ë¶ˆìš©ì–´ (utils.pyì—ì„œ ê°€ì ¸ì˜´)
LEGAL_STOPWORDS = [
    # ê¸°ë³¸ ë¶ˆìš©ì–´
    'ì œ', 'ê²ƒ', 'ë“±', 'ë•Œ', 'ê²½ìš°', 'ë°”', 'ìˆ˜', 'ì ', 'ë©´', 'ì´', 'ê·¸', 'ì €', 'ì€', 'ëŠ”', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ìœ¼ë¡œ',
    'ë”°ë¼', 'ë˜ëŠ”', 'ë°', 'ìˆë‹¤', 'í•œë‹¤', 'ë˜ì–´', 'ì¸í•œ', 'ëŒ€í•œ', 'ê´€í•œ', 'ìœ„í•œ', 'í†µí•œ', 'ê°™ì€', 'ë‹¤ë¥¸',

    # ë²•ë ¹ êµ¬ì¡° ë¶ˆìš©ì–´
    'ì¡°í•­', 'ê·œì •', 'ë²•ë¥ ', 'ë²•ë ¹', 'ì¡°ë¬¸', 'í•­ëª©', 'ì„¸ë¶€', 'ë‚´ìš©', 'ì‚¬í•­', 'ìš”ê±´', 'ê¸°ì¤€', 'ë°©ë²•', 'ì ˆì°¨',

    # ì¼ë°˜ì ì¸ ë™ì‚¬/í˜•ìš©ì‚¬
    'í•´ë‹¹', 'ê´€ë ¨', 'í¬í•¨', 'ì œì™¸', 'ì ìš©', 'ì‹œí–‰', 'ì¤€ìš©', 'ì˜í•˜ë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ê°™ë‹¤'
]

class VectorizationMethod:
    """ë²¡í„°í™” ë°©ì‹ ê¸°ë³¸ í´ë˜ìŠ¤"""

    def __init__(self, name: str):
        self.name = name
        self.vectorizers = []
        self.tfidf_matrices = []
        self.vectorization_time = 0
        self.memory_usage = 0
        self.vocab_size = 0

    def fit_transform(self, corpus: List[str]) -> None:
        """ì½”í¼ìŠ¤ë¥¼ ë²¡í„°í™”"""
        raise NotImplementedError

    def search(self, query: str, top_k: int = 10) -> Tuple[np.ndarray, np.ndarray]:
        """ì¿¼ë¦¬ ê²€ìƒ‰ (ì¸ë±ìŠ¤, ìœ ì‚¬ë„ ë°˜í™˜)"""
        raise NotImplementedError


class WordBasedVectorizer(VectorizationMethod):
    """Word-based TF-IDF (í˜„ì¬ ë°©ì‹)"""

    def __init__(self):
        super().__init__("Word-based")

    def fit_transform(self, corpus: List[str]) -> None:
        tracemalloc.start()
        start_time = time.time()

        self.vectorizer = TfidfVectorizer(
            analyzer='word',
            ngram_range=(1, 2),
            stop_words=LEGAL_STOPWORDS,
            min_df=1,
            max_df=0.8,
            sublinear_tf=True,
            use_idf=True,
            smooth_idf=True,
            norm='l2'
        )

        self.tfidf_matrix = self.vectorizer.fit_transform(corpus)

        self.vectorization_time = time.time() - start_time
        current, peak = tracemalloc.get_traced_memory()
        self.memory_usage = peak / 1024 / 1024  # MB
        tracemalloc.stop()

        self.vocab_size = len(self.vectorizer.vocabulary_)

    def search(self, query: str, top_k: int = 10) -> Tuple[np.ndarray, np.ndarray]:
        start_time = time.time()

        query_vec = self.vectorizer.transform([query])
        similarities = cosine_similarity(query_vec, self.tfidf_matrix)[0]
        top_indices = similarities.argsort()[-top_k:][::-1]
        top_scores = similarities[top_indices]

        search_time = time.time() - start_time
        return top_indices, top_scores, search_time


class CharNgramVectorizer(VectorizationMethod):
    """Character n-gram TF-IDF"""

    def __init__(self):
        super().__init__("Char n-gram")

    def fit_transform(self, corpus: List[str]) -> None:
        tracemalloc.start()
        start_time = time.time()

        self.vectorizer = TfidfVectorizer(
            analyzer='char',
            ngram_range=(2, 4),
            max_df=0.9,
            min_df=1,
            max_features=50000,  # ì°¨ì› í­ë°œ ë°©ì§€
            sublinear_tf=True,
            use_idf=True,
            smooth_idf=True,
            norm='l2'
        )

        self.tfidf_matrix = self.vectorizer.fit_transform(corpus)

        self.vectorization_time = time.time() - start_time
        current, peak = tracemalloc.get_traced_memory()
        self.memory_usage = peak / 1024 / 1024  # MB
        tracemalloc.stop()

        self.vocab_size = len(self.vectorizer.vocabulary_)

    def search(self, query: str, top_k: int = 10) -> Tuple[np.ndarray, np.ndarray]:
        start_time = time.time()

        query_vec = self.vectorizer.transform([query])
        similarities = cosine_similarity(query_vec, self.tfidf_matrix)[0]
        top_indices = similarities.argsort()[-top_k:][::-1]
        top_scores = similarities[top_indices]

        search_time = time.time() - start_time
        return top_indices, top_scores, search_time


class HybridVectorizer(VectorizationMethod):
    """Hybrid (Word 50% + Char 50%)"""

    def __init__(self, word_weight: float = 0.5):
        super().__init__(f"Hybrid (W:{word_weight:.0%}/C:{1-word_weight:.0%})")
        self.word_weight = word_weight
        self.char_weight = 1 - word_weight

    def fit_transform(self, corpus: List[str]) -> None:
        tracemalloc.start()
        start_time = time.time()

        # Word vectorizer
        self.word_vectorizer = TfidfVectorizer(
            analyzer='word',
            ngram_range=(1, 2),
            stop_words=LEGAL_STOPWORDS,
            min_df=1,
            max_df=0.8,
            sublinear_tf=True,
            use_idf=True,
            smooth_idf=True,
            norm='l2'
        )

        # Char vectorizer
        self.char_vectorizer = TfidfVectorizer(
            analyzer='char',
            ngram_range=(2, 4),
            max_df=0.9,
            min_df=1,
            max_features=50000,
            sublinear_tf=True,
            use_idf=True,
            smooth_idf=True,
            norm='l2'
        )

        self.word_matrix = self.word_vectorizer.fit_transform(corpus)
        self.char_matrix = self.char_vectorizer.fit_transform(corpus)

        self.vectorization_time = time.time() - start_time
        current, peak = tracemalloc.get_traced_memory()
        self.memory_usage = peak / 1024 / 1024  # MB
        tracemalloc.stop()

        self.vocab_size = len(self.word_vectorizer.vocabulary_) + len(self.char_vectorizer.vocabulary_)

    def search(self, query: str, top_k: int = 10) -> Tuple[np.ndarray, np.ndarray]:
        start_time = time.time()

        # Word similarity
        query_word_vec = self.word_vectorizer.transform([query])
        word_similarities = cosine_similarity(query_word_vec, self.word_matrix)[0]

        # Char similarity
        query_char_vec = self.char_vectorizer.transform([query])
        char_similarities = cosine_similarity(query_char_vec, self.char_matrix)[0]

        # Hybrid score
        similarities = self.word_weight * word_similarities + self.char_weight * char_similarities

        top_indices = similarities.argsort()[-top_k:][::-1]
        top_scores = similarities[top_indices]

        search_time = time.time() - start_time
        return top_indices, top_scores, search_time


class PerformanceEvaluator:
    """ì„±ëŠ¥ í‰ê°€ í´ë˜ìŠ¤"""

    @staticmethod
    def calculate_precision_at_k(retrieved: List[int], relevant: List[int], k: int) -> float:
        """Precision@K ê³„ì‚°"""
        if k == 0:
            return 0.0
        retrieved_k = retrieved[:k]
        relevant_retrieved = len(set(retrieved_k) & set(relevant))
        return relevant_retrieved / k

    @staticmethod
    def calculate_recall_at_k(retrieved: List[int], relevant: List[int], k: int) -> float:
        """Recall@K ê³„ì‚°"""
        if len(relevant) == 0:
            return 0.0
        retrieved_k = retrieved[:k]
        relevant_retrieved = len(set(retrieved_k) & set(relevant))
        return relevant_retrieved / len(relevant)

    @staticmethod
    def calculate_mrr(retrieved: List[int], relevant: List[int]) -> float:
        """Mean Reciprocal Rank ê³„ì‚°"""
        for i, doc_id in enumerate(retrieved):
            if doc_id in relevant:
                return 1.0 / (i + 1)
        return 0.0

    @staticmethod
    def calculate_ndcg_at_k(retrieved: List[int], relevant: List[int], k: int) -> float:
        """NDCG@K ê³„ì‚°"""
        dcg = 0.0
        for i, doc_id in enumerate(retrieved[:k]):
            if doc_id in relevant:
                dcg += 1.0 / np.log2(i + 2)  # i+2 because i starts from 0

        # Ideal DCG
        idcg = sum([1.0 / np.log2(i + 2) for i in range(min(len(relevant), k))])

        return dcg / idcg if idcg > 0 else 0.0


def preprocess_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
    if not text or not isinstance(text, str):
        return ""
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    return text


def extract_text_from_item(item: Dict, data_type: str) -> str:
    """ë°ì´í„° ì•„ì´í…œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (utils.pyì™€ ë™ì¼)"""
    if data_type == "court_case":
        text_parts = []
        for key in ['ì‚¬ê±´ë²ˆí˜¸', 'ì„ ê³ ì¼ì\n(ì¢…ê²°ì¼ì)', 'íŒê²°ì£¼ë¬¸', 'ì²­êµ¬ì·¨ì§€', 'íŒê²°ì´ìœ ']:
            if key in item and item[key]:
                sub_text = f'{key}: {item[key]} \n\n'
                text_parts.append(sub_text)
        return ' '.join(text_parts)
    else:  # moleg
        text_parts = []
        field_weights = {
            'ì œëª©': 0.0625,
            'íŒë¡€ë²ˆí˜¸': 0.0625,
            'ë‚´ìš©': 0.0625,
            'ì„ ê³ ì¼ì': 0.0625,
            'ë²•ì›ëª…': 0.0625,
            'ì‚¬ê±´ìœ í˜•': 0.0625,
            'íŒê²°ìš”ì§€': 0.5,
            'ì°¸ì¡°ì¡°ë¬¸': 0.0625,
            'íŒê²°ê²°ê³¼': 0.0625
        }

        for field, weight in field_weights.items():
            if field in item and item[field]:
                field_text = f'{field}: {item[field]} \n\n'
                repeat_count = max(1, int(weight * 10))
                for _ in range(repeat_count):
                    text_parts.append(field_text)

        return ' '.join(text_parts)


def load_test_data():
    """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ"""
    print("ë°ì´í„° ë¡œë”© ì¤‘...")

    with open("data_kcs.json", "r", encoding="utf-8") as f:
        court_cases = json.load(f)

    with open("data_moleg.json", "r", encoding="utf-8") as f:
        moleg_cases = json.load(f)

    # ì „ì²´ ë°ì´í„° í•©ì¹˜ê¸°
    all_data = court_cases + moleg_cases

    # ì½”í¼ìŠ¤ ìƒì„±
    corpus = []
    for i, item in enumerate(all_data):
        if i < len(court_cases):
            text = extract_text_from_item(item, "court_case")
        else:
            text = extract_text_from_item(item, "moleg")
        corpus.append(preprocess_text(text))

    print(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: KCS {len(court_cases)}ê±´ + MOLEG {len(moleg_cases)}ê±´ = ì´ {len(all_data)}ê±´")
    return all_data, corpus


def create_test_queries():
    """í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìƒì„±"""
    test_queries = [
        # ì •í™•í•œ ë²•ì¡°ë¬¸ ê²€ìƒ‰ (5ê°œ)
        {"query": "ê´€ì„¸ë²• ì œ241ì¡°", "type": "exact_match", "description": "ì •í™•í•œ ë²•ì¡°ë¬¸"},
        {"query": "ê´€ì„¸ë²• ì œ269ì¡° ë°€ìˆ˜ì…ì£„", "type": "exact_match", "description": "ë²•ì¡°ë¬¸+ì£„ëª…"},
        {"query": "ê´€ì„¸ í™˜ê¸‰", "type": "exact_match", "description": "ì¼ë°˜ ë²•ë¥  ìš©ì–´"},
        {"query": "ìˆ˜ì…ì‹ ê³ ", "type": "exact_match", "description": "ê´€ì„¸ ì ˆì°¨ ìš©ì–´"},
        {"query": "ê´€ì„¸ ë‚©ë¶€ ì˜ë¬´", "type": "exact_match", "description": "ì˜ë¬´ ê´€ë ¨"},

        # í˜•íƒœì†Œ ë³€í˜• (5ê°œ)
        {"query": "ê´€ì„¸ë²•ë ¹", "type": "morphology", "description": "ë²•+ë²•ë ¹ ë³€í˜•"},
        {"query": "ìˆ˜ì…ì‹ ê³ ì„œ", "type": "morphology", "description": "ì‹ ê³ +ì„œë¥˜"},
        {"query": "ê´€ì„¸í™˜ê¸‰ê¸ˆ", "type": "morphology", "description": "í™˜ê¸‰+ê¸ˆ"},
        {"query": "ë°€ìˆ˜ì¶œì…", "type": "morphology", "description": "ë°€ìˆ˜+ì¶œì…"},
        {"query": "ì„¸ê´€ê²€ì‚¬", "type": "morphology", "description": "ì„¸ê´€+ê²€ì‚¬"},

        # ë³µí•© ë²•ë¥  ìš©ì–´ (5ê°œ)
        {"query": "ê´€ì„¸í™˜ê¸‰ê¸ˆ ì²­êµ¬ ì ˆì°¨", "type": "complex_term", "description": "í™˜ê¸‰ ì ˆì°¨"},
        {"query": "ë°€ìˆ˜ì… ì²˜ë²Œ ê¸°ì¤€", "type": "complex_term", "description": "ì²˜ë²Œ ê¸°ì¤€"},
        {"query": "ìˆ˜ì…ì‹ ê³  ì˜ë¬´ ìœ„ë°˜", "type": "complex_term", "description": "ì˜ë¬´ ìœ„ë°˜"},
        {"query": "ê³¼ì„¸ê°€ê²© ê²°ì • ë°©ë²•", "type": "complex_term", "description": "ê³¼ì„¸ ë°©ë²•"},
        {"query": "ì›ì‚°ì§€ ì¦ëª…ì„œ ì œì¶œ", "type": "complex_term", "description": "ì¦ëª…ì„œ ì œì¶œ"},

        # ì˜¤íƒ€ í…ŒìŠ¤íŠ¸ (3ê°œ)
        {"query": "ê´€ì„¸ë±", "type": "typo", "description": "ë²•â†’ë± ì˜¤íƒ€"},
        {"query": "ìˆ˜ì…ì‹ ê³ ì„œ", "type": "typo", "description": "ì •ìƒ (ë¹„êµìš©)"},
        {"query": "ê´€ì„¸ì™„ê¸‰", "type": "typo", "description": "í™˜â†’ì™„ ì˜¤íƒ€"},

        # ë¶€ë¶„ ë§¤ì¹­ (2ê°œ)
        {"query": "ìˆ˜ì…ì‹ ê³ ", "type": "partial_match", "description": "ìˆ˜ì…ì‹ ê³ ì˜ë¬´ì ê²€ìƒ‰"},
        {"query": "ê´€ì„¸", "type": "partial_match", "description": "ê´€ì„¸ë²• ê´€ë ¨ ì „ì²´"},
    ]

    return test_queries


def run_tests():
    """ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("="*70)
    print("TF-IDF ë²¡í„°í™” ë°©ì‹ ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸")
    print("="*70)
    print()

    # ë°ì´í„° ë¡œë“œ
    all_data, corpus = load_test_data()

    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    test_queries = create_test_queries()
    print(f"í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {len(test_queries)}ê°œ\n")

    # 3ê°€ì§€ ë²¡í„°í™” ë°©ì‹
    methods = [
        WordBasedVectorizer(),
        CharNgramVectorizer(),
        HybridVectorizer(word_weight=0.5)
    ]

    # ê° ë°©ì‹ìœ¼ë¡œ ë²¡í„°í™”
    print("ë²¡í„°í™” ìˆ˜í–‰ ì¤‘...\n")
    for method in methods:
        print(f"  {method.name} ë²¡í„°í™” ì¤‘...", end=" ")
        method.fit_transform(corpus)
        print(f"ì™„ë£Œ ({method.vectorization_time:.2f}ì´ˆ)")

    print()

    # ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸”
    print("="*70)
    print("1. ë²¡í„°í™” ì„±ëŠ¥ ë¹„êµ")
    print("="*70)
    print(f"{'ë°©ì‹':<20} {'ë²¡í„°í™” ì‹œê°„':<15} {'ë²¡í„° ì°¨ì›':<15} {'ë©”ëª¨ë¦¬(MB)':<15}")
    print("-"*70)
    for method in methods:
        print(f"{method.name:<20} {method.vectorization_time:>10.2f}ì´ˆ    {method.vocab_size:>10,}ê°œ    {method.memory_usage:>10.1f} MB")
    print()

    # ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    print("="*70)
    print("2. ê²€ìƒ‰ ì •í™•ë„ ë¹„êµ")
    print("="*70)

    evaluator = PerformanceEvaluator()
    results = {method.name: {
        'precision@5': [],
        'recall@10': [],
        'mrr': [],
        'ndcg@10': [],
        'search_times': [],
        'query_results': []
    } for method in methods}

    # ê° ì¿¼ë¦¬ì— ëŒ€í•´ í…ŒìŠ¤íŠ¸
    for i, test_query in enumerate(test_queries):
        query = test_query['query']
        query_type = test_query['type']

        if i % 5 == 0:
            print(f"  ì§„í–‰: {i}/{len(test_queries)} ì¿¼ë¦¬...", end="\r")

        # ê° ë°©ì‹ìœ¼ë¡œ ê²€ìƒ‰
        method_results = {}
        for method in methods:
            indices, scores, search_time = method.search(query, top_k=10)
            method_results[method.name] = {
                'indices': indices,
                'scores': scores,
                'search_time': search_time
            }
            results[method.name]['search_times'].append(search_time)

        # Ground truth ìƒì„± (ê°„ë‹¨í•œ ë°©ë²•: ëª¨ë“  ë°©ì‹ì˜ ìƒìœ„ ê²°ê³¼ í•©ì§‘í•©ì—ì„œ ì ìˆ˜ ë†’ì€ ê²ƒ)
        # ì‹¤ì œë¡œëŠ” ìˆ˜ë™ ë¼ë²¨ë§ì´ í•„ìš”í•˜ì§€ë§Œ, ìë™ í‰ê°€ë¥¼ ìœ„í•´ íœ´ë¦¬ìŠ¤í‹± ì‚¬ìš©
        all_top_indices = set()
        for method_name, result in method_results.items():
            # ìœ ì‚¬ë„ > 0.1ì¸ ê²ƒë§Œ ê´€ë ¨ ë¬¸ì„œë¡œ ê°„ì£¼
            relevant_indices = result['indices'][result['scores'] > 0.1]
            all_top_indices.update(relevant_indices.tolist())

        # ê° ë°©ì‹ì˜ ì •í™•ë„ ê³„ì‚°
        for method_name, result in method_results.items():
            indices = result['indices'].tolist()
            relevant = list(all_top_indices)

            precision = evaluator.calculate_precision_at_k(indices, relevant, 5)
            recall = evaluator.calculate_recall_at_k(indices, relevant, 10)
            mrr = evaluator.calculate_mrr(indices, relevant)
            ndcg = evaluator.calculate_ndcg_at_k(indices, relevant, 10)

            results[method_name]['precision@5'].append(precision)
            results[method_name]['recall@10'].append(recall)
            results[method_name]['mrr'].append(mrr)
            results[method_name]['ndcg@10'].append(ndcg)

            results[method_name]['query_results'].append({
                'query': query,
                'type': query_type,
                'precision': precision,
                'recall': recall,
                'top_score': result['scores'][0] if len(result['scores']) > 0 else 0
            })

    print(f"  ì§„í–‰: {len(test_queries)}/{len(test_queries)} ì¿¼ë¦¬... ì™„ë£Œ!")
    print()

    # í‰ê·  ì§€í‘œ ì¶œë ¥
    print(f"{'ë°©ì‹':<20} {'Precision@5':<15} {'Recall@10':<15} {'MRR':<15} {'NDCG@10':<15} {'ê²€ìƒ‰ì†ë„(ms)':<15}")
    print("-"*100)
    for method_name in results:
        avg_precision = np.mean(results[method_name]['precision@5'])
        avg_recall = np.mean(results[method_name]['recall@10'])
        avg_mrr = np.mean(results[method_name]['mrr'])
        avg_ndcg = np.mean(results[method_name]['ndcg@10'])
        avg_search_time = np.mean(results[method_name]['search_times']) * 1000  # ms

        print(f"{method_name:<20} {avg_precision:>10.3f}      {avg_recall:>10.3f}      {avg_mrr:>10.3f}      {avg_ndcg:>10.3f}      {avg_search_time:>10.2f} ms")

    print()

    # ì¿¼ë¦¬ ìœ í˜•ë³„ ì„±ëŠ¥
    print("="*70)
    print("3. ì¿¼ë¦¬ ìœ í˜•ë³„ í‰ê·  ì •í™•ë„ (Precision@5)")
    print("="*70)

    query_types = set([q['type'] for q in test_queries])

    print(f"{'ìœ í˜•':<20} ", end="")
    for method_name in results:
        print(f"{method_name:<20} ", end="")
    print()
    print("-"*100)

    for qtype in query_types:
        print(f"{qtype:<20} ", end="")
        for method_name in results:
            type_precisions = [r['precision'] for r in results[method_name]['query_results'] if r['type'] == qtype]
            avg = np.mean(type_precisions) if type_precisions else 0
            print(f"{avg:>10.3f}          ", end="")
        print()

    print()

    # ìµœì¢… ì¶”ì²œ
    print("="*70)
    print("4. ìµœì¢… ì¶”ì²œ")
    print("="*70)

    # ì¢…í•© ì ìˆ˜ ê³„ì‚° (Precision, Recall, MRR, NDCG í‰ê· )
    composite_scores = {}
    for method_name in results:
        score = (
            np.mean(results[method_name]['precision@5']) * 0.3 +
            np.mean(results[method_name]['recall@10']) * 0.3 +
            np.mean(results[method_name]['mrr']) * 0.2 +
            np.mean(results[method_name]['ndcg@10']) * 0.2
        )
        composite_scores[method_name] = score

    # ìµœê³  ì ìˆ˜
    best_method = max(composite_scores, key=composite_scores.get)
    best_score = composite_scores[best_method]

    print(f"ì¢…í•© ì ìˆ˜ (ê°€ì¤‘ í‰ê· ):")
    for method_name, score in sorted(composite_scores.items(), key=lambda x: x[1], reverse=True):
        star = " â­ ì¶”ì²œ!" if method_name == best_method else ""
        print(f"  {method_name:<20} {score:.3f}{star}")

    print()
    print(f"ìµœì¢… ì¶”ì²œ ë°©ì‹: {best_method}")
    print()

    # ë°©ì‹ë³„ íŠ¹ì§•
    print("ê° ë°©ì‹ì˜ íŠ¹ì§•:")
    print("-" * 70)

    method_idx = list(results.keys()).index(best_method)
    best_method_obj = methods[method_idx]

    print(f"\nğŸ† {best_method}")
    print(f"  - ì¢…í•© ì ìˆ˜: {best_score:.3f}")
    print(f"  - Precision@5: {np.mean(results[best_method]['precision@5']):.3f}")
    print(f"  - Recall@10: {np.mean(results[best_method]['recall@10']):.3f}")
    print(f"  - ë²¡í„°í™” ì‹œê°„: {best_method_obj.vectorization_time:.2f}ì´ˆ")
    print(f"  - ê²€ìƒ‰ ì†ë„: {np.mean(results[best_method]['search_times'])*1000:.2f}ms")
    print(f"  - ë©”ëª¨ë¦¬ ì‚¬ìš©: {best_method_obj.memory_usage:.1f}MB")

    if "Word" in best_method:
        print(f"  - ì¥ì : ë¹ ë¥¸ ì†ë„, ì •í™•í•œ ë²•ë¥  ìš©ì–´ ë§¤ì¹­, ë‚®ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©")
        print(f"  - ë‹¨ì : í˜•íƒœì†Œ ë³€í˜•/ì˜¤íƒ€ì— ì·¨ì•½")
        print(f"  - ê¶Œì¥: ì •í™•í•œ ë²•ì¡°ë¬¸ ê²€ìƒ‰ ì¤‘ì‹¬ í™˜ê²½")
    elif "Char" in best_method:
        print(f"  - ì¥ì : í˜•íƒœì†Œ ë³€í˜• ìë™ ì¸ì‹, ì˜¤íƒ€ ê°•ê±´ì„±, ë¶€ë¶„ ë§¤ì¹­ ìš°ìˆ˜")
        print(f"  - ë‹¨ì : ë†’ì€ ë©”ëª¨ë¦¬/ê³„ì‚° ë¹„ìš©, ì •í™•ë„ ë‹¤ì†Œ ë‚®ìŒ")
        print(f"  - ê¶Œì¥: ì‚¬ìš©ì ì…ë ¥ í’ˆì§ˆì´ ë‚®ì€ í™˜ê²½")
    else:  # Hybrid
        print(f"  - ì¥ì : ì •í™•ë„ì™€ ì¬í˜„ìœ¨ ê· í˜•, ë‹¤ì–‘í•œ ì¿¼ë¦¬ ëŒ€ì‘")
        print(f"  - ë‹¨ì : ê°€ì¥ ë†’ì€ ë©”ëª¨ë¦¬/ê³„ì‚° ë¹„ìš©")
        print(f"  - ê¶Œì¥: ì •í™•ë„ê°€ ì¤‘ìš”í•œ í”„ë¡œë•ì…˜ í™˜ê²½")

    print()
    print("="*70)
    print("í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*70)

    return results, methods


if __name__ == "__main__":
    import sys

    try:
        results, methods = run_tests()
    except FileNotFoundError as e:
        print(f"\nì˜¤ë¥˜: ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"data_kcs.jsonê³¼ data_moleg.json íŒŒì¼ì´ í˜„ì¬ ë””ë ‰í† ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        sys.exit(1)
    except Exception as e:
        print(f"\nì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
